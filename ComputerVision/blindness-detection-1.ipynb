{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports\n\n* Instead of original dataset provided, we work with combination of (2019+2015) dataset\n* Instead of .png , dataset uses .jpeg pictures\n* The dataset creators also ensures that they have eliminated the ultra-dark images\n* COMPETITION DATASET : 5590 images (aptos2019-blindness-detection)\n* CONTRIBUTED DATASET : ~ 35k images (diabetic-retinopathy-resized)\n* special thanks to : @ILOVESCIENCE for this dataset\n\nhttps://www.kaggle.com/datasets/tanlikesmath/diabetic-retinopathy-resized?select=trainLabels_cropped.csv\n\nNOTE : **Quadratic Kappa Metric** is the same as **cohen kappa metric** in Sci-kit learn @ sklearn.metrics.cohen_kappa_score when **weights = 'Quadratic'**. ","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom tqdm import tqdm \nimport warnings\nwarnings.filterwarnings('ignore')\nimport time\n\n# Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\n\n# sklearn\nfrom sklearn.utils import class_weight, shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\nimport sklearn.utils.class_weight as class_weight\n\n# torch\nimport torchvision\nfrom torchvision import transforms\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torchmetrics\n\n# lightening\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar, RichModelSummary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-20T05:30:14.010632Z","iopub.execute_input":"2023-10-20T05:30:14.011089Z","iopub.status.idle":"2023-10-20T05:30:14.017536Z","shell.execute_reply.started":"2023-10-20T05:30:14.011035Z","shell.execute_reply":"2023-10-20T05:30:14.016570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysing Data","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/diabetic-retinopathy-resized/trainLabels_cropped.csv')\ntest_data = pd.read_csv('/kaggle/input/aptos2019-blindness-detection/test.csv')\n\ntrain_data.rename(columns={'level': 'labels'}, inplace=True)\ntrain_data = train_data.drop([\"Unnamed: 0.1\", \"Unnamed: 0\"], axis = 1)\n\nclass_labels = train_data['labels'].unique() \nprint(\"Target classes: {}\".format(class_labels))\n\ntrain_data['eye_type'] = train_data['image'].map(lambda x: x.split('_')[1])\ntrain_data.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:14.022726Z","iopub.execute_input":"2023-10-20T05:30:14.023001Z","iopub.status.idle":"2023-10-20T05:30:14.086929Z","shell.execute_reply.started":"2023-10-20T05:30:14.022979Z","shell.execute_reply":"2023-10-20T05:30:14.086090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_counts = train_data['labels'].value_counts()\nclass_labels = class_counts.index.tolist()\nclass_sizes = class_counts.tolist()\n\nclass_explode = [0.1] * len(class_labels)  \n\nfig, axs = plt.subplots(1, 2, figsize=(14, 7))\n\n# Subplot 1: Distribution of Training and Testing Data\naxs[0].pie([len(train_data), len(test_data)], explode=[0, 0.1], labels=['Train Data', 'Test Data'], shadow=True, autopct='%1.1f%%', startangle=100)\naxs[0].set_title('Distribution of Test and Training Data')\n\n# Subplot 2: Distribution of Classes in Training Data\naxs[1].pie(class_sizes, explode=class_explode, labels=class_labels, shadow=True, autopct='%1.1f%%', startangle=45)\naxs[1].set_title('Distribution of Classes in Training Data')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:14.088340Z","iopub.execute_input":"2023-10-20T05:30:14.088618Z","iopub.status.idle":"2023-10-20T05:30:14.328367Z","shell.execute_reply.started":"2023-10-20T05:30:14.088594Z","shell.execute_reply":"2023-10-20T05:30:14.327519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*As we can see data is highly imbalanced and we have approx 10 times more images for class 0 than other classes*\n\n* We must use weighted loss, to weigh the loss of small classes more than the dominant one","metadata":{}},{"cell_type":"code","source":"tr_path = '/kaggle/input/diabetic-retinopathy-resized/resized_train_cropped/resized_train_cropped'\ntst_path = '/kaggle/input/aptos2019-blindness-detection/test_images'\n\ntrain_data['path'] = train_data['image'].map(lambda x: os.path.join(tr_path,'{}.jpeg'.format(x)))\ntest_data['path'] = test_data['id_code'].map(lambda x: os.path.join(tst_path,'{}.png'.format(x)))\n\nprint(train_data['eye_type'].value_counts())\ntrain_data = train_data.iloc[:len(train_data) // 2]\nprint(train_data.shape)\ntrain_data.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:14.329648Z","iopub.execute_input":"2023-10-20T05:30:14.330172Z","iopub.status.idle":"2023-10-20T05:30:14.394450Z","shell.execute_reply.started":"2023-10-20T05:30:14.330141Z","shell.execute_reply":"2023-10-20T05:30:14.393604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.to_csv('tr_df.csv', index=  False)\ntest_data.to_csv('tst_df.csv', index=  False)","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:35:03.717915Z","iopub.execute_input":"2023-10-20T05:35:03.718269Z","iopub.status.idle":"2023-10-20T05:35:03.796409Z","shell.execute_reply.started":"2023-10-20T05:35:03.718233Z","shell.execute_reply":"2023-10-20T05:35:03.795708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Plotting ","metadata":{}},{"cell_type":"code","source":"N = 12\nrandom_idxs = np.random.choice(train_data.index , N)\n\nfig, axs = plt.subplots(4,3,figsize = (15,20))\nfor i in range(N):\n    r,c = i//3 , i%3\n    idx = random_idxs[i]\n    img_path, eye_type = train_data.iloc[idx, 3],  train_data.iloc[idx, 2]\n    img = cv2.imread(img_path)\n    axs[r,c].imshow(img)\n    axs[r,c].set_title('{a}_{b}'.format(a=img.shape , b = eye_type))\n    axs[r,c].set_xticks([])\n    axs[r,c].set_yticks([])\n    \nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:14.396659Z","iopub.execute_input":"2023-10-20T05:30:14.397163Z","iopub.status.idle":"2023-10-20T05:30:18.956204Z","shell.execute_reply.started":"2023-10-20T05:30:14.397131Z","shell.execute_reply":"2023-10-20T05:30:18.954812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformations & Splitting\n\nThe images are of different shapes, they can't be batched directly. Hence we can apply the following transforms :\n\n* resize_with_pad \n* Centre_crop\n* We can't apply vertical_flip , as it will not match reality\n* We need not to apply horizontal_flip, as there are enough no. of (left_eye ,right_eye) images\n* We can apply affine transformations for model robustness.\n\nWe can skip the first 2 transformations as every pre-trained model do these transformations internally.\n","metadata":{}},{"cell_type":"code","source":"img_dim = 600\nimg_transforms = transforms.Compose([\n        torchvision.transforms.Resize(size=(img_dim,img_dim)), \n        torchvision.transforms.GaussianBlur(kernel_size = 5, sigma = (20,20)),\n        transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n        transforms.ToTensor(),\n   ])\n\n# Creating train and validation sets\n\nX, y = train_data.path, train_data.labels\nX_tr, X_val, Y_tr,Y_val = train_test_split(X, y, test_size=0.2,\n                                                      stratify=y, random_state=42)\nX_tr.reset_index(drop = True, inplace = True)\nY_tr.reset_index(drop = True, inplace = True)\nX_val.reset_index(drop = True, inplace = True)\nY_val.reset_index(drop = True, inplace = True)\nprint(X_tr.shape, Y_tr.shape, X_val.shape, Y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:18.957298Z","iopub.execute_input":"2023-10-20T05:30:18.957532Z","iopub.status.idle":"2023-10-20T05:30:18.975309Z","shell.execute_reply.started":"2023-10-20T05:30:18.957511Z","shell.execute_reply":"2023-10-20T05:30:18.974472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Preprocessing","metadata":{}},{"cell_type":"code","source":"# # Plotting pixel_histogram of pixels of each channel\n\n# img = cv2.imread(X_tr.iloc[0])\n# img_channel_list = [img , img[:,:,0] , img[:,:,1] ,img[:,:,2]]\n# fig, axs = plt.subplots(2,2 , figsize= (15,8))\n\n# axs[0,0].imshow(img)\n# axs[0,0].set_xticks([])\n# axs[0,0].set_yticks([])\n# labels = [i for i in range (256)]\n# for i in range(1,4):\n#     r,c  = i//2,i%2\n#     axs[r,c].hist(img_channel_list[i])\n    \n#     axs[r,c].set_xticks(labels, labels, rotation=45)\n# #     axs[r,c].set_xticklabels([i for i in range (256)], rotation=45)\n#     axs[r,c].set_yticks([])\n#     axs[r,c].set_title('Channel no. {x}'.format(x=i-1))","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:18.976385Z","iopub.execute_input":"2023-10-20T05:30:18.976610Z","iopub.status.idle":"2023-10-20T05:30:18.983617Z","shell.execute_reply.started":"2023-10-20T05:30:18.976590Z","shell.execute_reply":"2023-10-20T05:30:18.982854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, each channel has clear distinction between low pixel values and rest pixel values (0_th and 1_st towers in each channel)\n\nSo, even if we merge these channels, rest small towers may be compensated by other channels, but the distinction between 0_th and 1_st channel stays\n\nHence, we can use gray_scale image for getting img_contours for cropping","metadata":{}},{"cell_type":"code","source":"class PreProcessing:\n    def __init__(self, no_channels, tol=7, sigmaX=30):\n        self.no_channels = no_channels\n        self.tol = tol\n        self.sigmaX = sigmaX\n\n    def cropping_2D(self, img, is_cropping = False):    # for Cropping the extra dark part of the GRAY images\n\n        mask = img>self.tol      #  thresholding the img at a value of self.tol\n        '''\n         --> If the mask image has any pixels set to 1 in a row, then the corresponding row in img will be kept. \n         --> If the mask image has any pixels set to 1 in a column, then the corresponding column in img will be kept. \n         --> Otherwise, the row or column in the input image will be removed.\n        '''\n        return img[np.ix_(mask.any(1),mask.any(0))]\n\n    def cropping_3D(self, img, is_cropping = False):  # for Cropping the extra dark part of the RGB images\n\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>self.tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # if image is too dark we return the image\n            return img \n        else:\n            img1 = img[:,:,0][np.ix_(mask.any(1),mask.any(0))]  #for channel_1 (R)\n            img2 = img[:,:,1][np.ix_(mask.any(1),mask.any(0))]  #for channel_2 (G)\n            img3 = img[:,:,2][np.ix_(mask.any(1),mask.any(0))]  #for channel_3 (B)         \n            img = np.stack([img1,img2,img3],axis=-1)\n        return img\n\n    def Gaussian_blur(self, img, is_gaussianblur = False):\n        # adding Gaussian blur (image smoothing technique) thus reducing noise in image.\n\n        return cv2.addWeighted ( img,4, cv2.GaussianBlur( img , (0,0) , self.sigmaX) ,-4 ,128)\n\n    def draw_circle(self,img, is_drawcircle = True):\n        #This function is used for drawing a circle from the center of the image.\n\n        x = int(self.img_width/2)\n        y = int(self.img_height/2)\n        r = np.amin((x,y))     # finding radius to draw a circle from the center of the image\n        circle_img = np.zeros((img_height, img_width), np.uint8)\n        cv2.circle(circle_img, (x,y), int(r), 1, thickness=-1)\n        img = cv2.bitwise_and(img, img, mask=circle_img)\n        return img\n\n    def image_preprocessing(self, img, is_cropping = True, is_gaussianblur = True):\n#         if img.shape[2] == 2:\n#             img = self.cropping_2D(img, is_cropping)  #calling cropping_2D for a GRAY image\n#         else:\n        img = self.cropping_3D(img, is_cropping)  #calling cropping_3D for a RGB image\n#         img = cv2.resize(img, (self.img_height, self.img_width))  # resizing the image with specified values\n#         img = self.draw_circle(img)  #calling draw_circle\n        img = self.Gaussian_blur(img, is_gaussianblur) #calling Gaussian_blur\n        return img","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:18.984877Z","iopub.execute_input":"2023-10-20T05:30:18.985285Z","iopub.status.idle":"2023-10-20T05:30:19.001902Z","shell.execute_reply.started":"2023-10-20T05:30:18.985258Z","shell.execute_reply":"2023-10-20T05:30:19.000947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# channels = 3\n# split_size = 0.2\n# class_labels = {0: 'No DR[0]',1: 'Mild[1]', 2: 'Moderate[2]', 3: 'Severe[3]', 4: 'Proliferative DR[4]'}\n# obj = PreProcessing(channels, sigmaX = 45)\n# random_img_paths = np.random.choice(X_tr, 3)\n\n# fig, axs = plt.subplots(3,2, figsize = (15,15))\n\n# for i in range(3):\n#     raw_img = cv2.imread(random_img_paths[i])\n#     axs[i,0].imshow(raw_img)\n#     axs[i,0].set_title('unprocessed : {x}'.format(x = raw_img.shape))\n#     axs[i,0].set_xticks([])\n#     axs[i,0].set_yticks([])\n    \n#     processed_img = obj.image_preprocessing(raw_img)\n#     axs[i,1].imshow(processed_img)\n#     axs[i,1].set_title('processed :{x}'.format(x = processed_img.shape))\n#     axs[i,1].set_xticks([])\n#     axs[i,1].set_yticks([])\n    \n# plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:19.002909Z","iopub.execute_input":"2023-10-20T05:30:19.003153Z","iopub.status.idle":"2023-10-20T05:30:19.017789Z","shell.execute_reply.started":"2023-10-20T05:30:19.003133Z","shell.execute_reply":"2023-10-20T05:30:19.017026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Input PipeLine","metadata":{}},{"cell_type":"code","source":"class_weights = class_weight.compute_class_weight(class_weight= 'balanced',classes=  np.unique(Y_tr), y= Y_tr)\nclass_weights = torch.tensor(class_weights,dtype=torch.float)\n\nclass Classifier(pl.LightningModule):\n    def __init__(self, model_obj):\n        super().__init__()\n        self.model = model_obj.model\n        self.config = model_obj.config\n        self.layer_lr = model_obj.layer_lr\n        \n        self.kappa = torchmetrics.classification.MulticlassCohenKappa(num_classes = self.config['num_classes'], weights = 'quadratic')\n        self.accuracy = torchmetrics.Accuracy(task = 'multiclass' , num_classes = self.config['num_classes'])\n        self.criterion = torch.nn.CrossEntropyLoss(weight = class_weights)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = self.criterion(y_hat, y.long())\n        self.accuracy(y_hat, y)\n        self.kappa(y_hat, y)\n        self.log(\"train_acc\", self.accuracy, on_epoch=True,prog_bar=True, logger=True)\n        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"train_kappa\", self.kappa, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = self.criterion(y_hat, y.long())\n        self.accuracy(y_hat, y)\n        self.kappa(y_hat, y)\n        self.log(\"val_acc\", self.accuracy, on_epoch=True,prog_bar=True, logger=True)\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"val_kappa\", self.kappa, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n  \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = self.criterion(y_hat, y)\n        self.accuracy(y_hat, y)\n        self.kappa(y_hat, y)\n        self.log(\"test_acc\", self.accuracy, on_epoch=True,prog_bar=True, logger=True)\n        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"test_kappa\", self.kappa, on_epoch=True, prog_bar=True, logger=True)\n        return loss\n  \n    def configure_optimizers(self):\n        optim =  torch.optim.Adam(self.layer_lr, lr = self.config['lr'])   # https://pytorch.org/docs/stable/optim.html\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=3, factor=0.5, threshold=0.001, cooldown =2,verbose=True)\n        return [optim], [{'scheduler': lr_scheduler, 'interval': 'epoch', 'monitor': 'train_loss', 'name': 'lr_scheduler'}]\n\n#___________________________________________________________________________________________________________________\nclass MyDataset(Dataset):\n    \n    # defining values in the constructor\n    def __init__(self , X,y,img_transforms, img_processing_obj, apply_processing = True):\n        self.X = X\n        self.Y = torch.tensor( y.values, dtype=torch.float32)\n        self.total = len(self.X)\n        self.img_transforms = img_transforms\n        self.img_processing_obj = img_processing_obj\n        self.apply_processing = apply_processing\n    \n    # Getting the data samples\n    def __getitem__(self, idx):\n        y =  self.Y[idx]\n        img_path = self.X.iloc[idx]\n        img_tensor = Image.open(img_path)   # np.asarray()\n\n#         if self.apply_processing:\n#             img_tensor = self.img_processing_obj.image_preprocessing(img_tensor)\n\n        img_tensor = self.img_transforms(img_tensor)\n        return img_tensor, y\n  \n    def __len__(self):\n        return self.total\n  \n            \n","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:19.018949Z","iopub.execute_input":"2023-10-20T05:30:19.019240Z","iopub.status.idle":"2023-10-20T05:30:19.040117Z","shell.execute_reply.started":"2023-10-20T05:30:19.019218Z","shell.execute_reply":"2023-10-20T05:30:19.039377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataLoaders","metadata":{}},{"cell_type":"code","source":"obj = PreProcessing(3, sigmaX = 45)\n\ntr_dataset = MyDataset(X_tr, Y_tr, img_transforms, obj)\nval_dataset = MyDataset(X_val, Y_val, img_transforms , obj)\n\nnum_workers = 4\nBATCH_SIZE = 32\ntr_loader = DataLoader(tr_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:19.043246Z","iopub.execute_input":"2023-10-20T05:30:19.043536Z","iopub.status.idle":"2023-10-20T05:30:19.050636Z","shell.execute_reply.started":"2023-10-20T05:30:19.043511Z","shell.execute_reply":"2023-10-20T05:30:19.049927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Callbacks","metadata":{}},{"cell_type":"code","source":"early_stop_callback = EarlyStopping(\n   monitor='val_loss',\n   min_delta=0.00,\n   patience=5,\n   verbose=True,\n   mode='min'\n)\n\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_loss',\n    save_top_k=2,\n    verbose=True,\n )\nrich_progress_bar = RichProgressBar()\n\nrich_model_summary = RichModelSummary(max_depth=2)","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:19.051982Z","iopub.execute_input":"2023-10-20T05:30:19.052429Z","iopub.status.idle":"2023-10-20T05:30:19.065195Z","shell.execute_reply.started":"2023-10-20T05:30:19.052390Z","shell.execute_reply":"2023-10-20T05:30:19.064341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transfer Learning\n\nNow is te time to do transfer_learning, here is my plan for code distribution\n\n* I create seperate object for each pre-trained model customised for the setting of that model\n* I do transfer-learning on each object, then, do analysis of models against the class they are weak/strong at configuring\n* Then do ensembling, to further enhance the results\n\n**Models to be analysed : ResNeXt-50 , XceptionNet, ViT**","metadata":{}},{"cell_type":"code","source":"class ResNeXt():\n  # https://pytorch.org/vision/main/models/generated/torchvision.models.resnet101.html\n    def __init__(self, config ):\n        self.config = config\n        self.resnext =  torchvision.models.resnet101(weights = 'ResNet101_Weights.DEFAULT', progress = True)\n        self.base_model = nn.Sequential(*list(self.resnext.children())[:-1])\n\n        self.__create_model__()\n        self.layer_lr = [{'params' : self.base_model.parameters()},{'params': self.head.parameters(), 'lr': self.config['lr'] * 100}]\n\n    def __create_model__(self):\n        self.head = nn.Sequential(\n                    Dense(0.2 , 2048 ,1024), \n                    Dense(0.15, 1024, 512) ,\n                    Dense(0, 512, self.config['num_classes'])\n        )\n        self.model = nn.Sequential(\n                    self.base_model ,\n                    nn.Flatten(), \n                    self.head ,\n                        )\n    def forward(self, x):\n        return self.model(x)\n#_______________________________________________________________________________________________________________\nclass EffecientNet():\n  # https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_v2_l.html#torchvision.models.efficientnet_v2_l\n    def __init__(self, config):\n        self.config = config\n        self.enet = torchvision.models.efficientnet_v2_l( weights='DEFAULT' , progress = True)    # 'DEFAULT'  : 'IMAGENET1K_V1'\n        self.base_model = nn.Sequential(*list(self.enet.children())[:-1])\n\n        self.__create_model__()\n        self.layer_lr = [{'params' : self.base_model.parameters()},{'params': self.head.parameters(), 'lr': self.config['lr'] * 100}]\n\n    def __create_model__(self): \n        self.head = nn.Sequential(\n                    Dense(0.4 , 1280 ,512), \n                    Dense(0, 512, self.config['num_classes'])\n        )\n        self.model = nn.Sequential(\n                      self.base_model ,\n                      self.head\n                            )      \n    def forward(self, x):\n        x =  self.model(x) \n        return x\n\n#__________________________________________________________________________________________________________________\n\nclass ViT():\n  # https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_v2_l.html#torchvision.models.efficientnet_v2_l\n    def __init__(self, config):\n        self.config = config\n        self.vit = torchvision.models.vit_b_32( weights='DEFAULT' , progress = True)    # 'DEFAULT'  : 'IMAGENET1K_V1'\n        self.base_model = nn.Sequential(*list(self.vit.children())[:-1])\n\n        self.__create_model__()\n        self.layer_lr = [{'params' : self.base_model.parameters()},{'params': self.head.parameters(), 'lr': self.config['lr'] * 100}]\n\n    def __create_model__(self): \n        self.head = nn.Sequential(\n                    Dense(0.2 , 768 ,512), \n                    Dense(0, 512, self.config['num_classes'])\n        )\n        self.model = nn.Sequential(\n                      self.base_model ,\n                      self.head\n                            )      \n    def forward(self, x):\n        x =  self.model(x) \n        return x\n#___________________________________________________________________________________________________________________\n\nclass Dense(nn.Module):\n    def __init__(self, drop ,in_size, out_size):\n        super(Dense ,self).__init__()\n        self.dropout = nn.Dropout(drop)\n        self.linear = nn.Linear(in_size, out_size)\n        self.prelu = nn.PReLU()\n\n    def forward(self, x):\n        x = self.dropout(x)\n        x = self.linear(x)\n        x = self.prelu(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:19.066602Z","iopub.execute_input":"2023-10-20T05:30:19.066974Z","iopub.status.idle":"2023-10-20T05:30:19.085998Z","shell.execute_reply.started":"2023-10-20T05:30:19.066945Z","shell.execute_reply":"2023-10-20T05:30:19.085209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ResNeXt-50","metadata":{}},{"cell_type":"code","source":"resNeXt_dir = os.path.join('/kaggle/working/' , 'resNeXt_50')\nif not os.path.isdir(resNeXt_dir):\n    os.mkdir(resNeXt_dir)\n    \nconfig = {\n    'dir' : resNeXt_dir, \n\n    'num_classes' : 5,\n\n    'BATCH_SIZE' : 32,\n    'lr' : 0.00001,\n\n    'ckpt_file_name' : '{epoch}-{val_loss:.2f}-{val_acc:.2f}-{val_kappa:.2f}',\n    'model_name' : 'resNeXt_50',\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:19.086928Z","iopub.execute_input":"2023-10-20T05:30:19.087182Z","iopub.status.idle":"2023-10-20T05:30:19.101193Z","shell.execute_reply.started":"2023-10-20T05:30:19.087162Z","shell.execute_reply":"2023-10-20T05:30:19.100374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_obj = ResNeXt(config)\nmodel = Classifier(model_obj)\n\ncheckpoint_callback.dirpath = os.path.join(config['dir'], 'ckpts')\ncheckpoint_callback.filename = config['ckpt_file_name']\n\nlogger = TensorBoardLogger(resNeXt_dir+\"/tb_logs\")\n\ntrainer = Trainer(callbacks=[early_stop_callback, checkpoint_callback, rich_progress_bar, rich_model_summary], \n                  accelerator = 'gpu' ,max_epochs=1, logger=[logger])  \n \ntrainer.fit(model, tr_loader, val_loader)","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:19.102209Z","iopub.execute_input":"2023-10-20T05:30:19.102446Z","iopub.status.idle":"2023-10-20T05:30:43.930965Z","shell.execute_reply.started":"2023-10-20T05:30:19.102426Z","shell.execute_reply":"2023-10-20T05:30:43.929540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EfficientNet_v2","metadata":{}},{"cell_type":"code","source":"efficientnet_v2_dir = os.path.join('/kaggle/working/' , 'efficientnet_v2')\nif not os.path.isdir(efficientnet_v2_dir):\n    os.mkdir(efficientnet_v2_dir)\n    \nconfig = {\n    'dir' : efficientnet_v2_dir, \n\n    'num_classes' : 5,\n\n    'BATCH_SIZE' : 32,\n    'lr' : 0.00001,\n\n    'ckpt_file_name' : '{epoch}-{val_loss:.2f}-{val_acc:.2f}-{val_kappa:.2f}',\n    'model_name' : 'efficientnet_v2',\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:43.932080Z","iopub.status.idle":"2023-10-20T05:30:43.932388Z","shell.execute_reply.started":"2023-10-20T05:30:43.932241Z","shell.execute_reply":"2023-10-20T05:30:43.932257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_obj = EffecientNet(config)\nmodel = Classifier(model_obj)\n\ncheckpoint_callback.dirpath = os.path.join(config['dir'], 'ckpts')\ncheckpoint_callback.filename = config['ckpt_file_name']\n\nlogger = TensorBoardLogger(efficientnet_v2_dir+\"/tb_logs\")\n\ntrainer = Trainer(callbacks=[early_stop_callback, checkpoint_callback, rich_progress_bar, rich_model_summary], \n                  accelerator = 'gpu' ,max_epochs=1, logger=[logger])  \n \ntrainer.fit(model, tr_loader, val_loader)","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:43.933359Z","iopub.status.idle":"2023-10-20T05:30:43.933643Z","shell.execute_reply.started":"2023-10-20T05:30:43.933508Z","shell.execute_reply":"2023-10-20T05:30:43.933522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ViT-b32","metadata":{}},{"cell_type":"code","source":"vit_b_32_dir = os.path.join('/kaggle/working/' , 'efficientnet_v2')\nif not os.path.isdir(vit_b_32_dir):\n    os.mkdir(vit_b_32_dir)\n    \nconfig = {\n    'dir' : vit_b_32_dir, \n\n    'num_classes' : 5, \n\n    'BATCH_SIZE' : 32, \n    'lr' : 0.00001, \n\n    'ckpt_file_name' : '{epoch}-{val_loss:.2f}-{val_acc:.2f}-{val_kappa:.2f}' ,\n    'model_name' : 'vit_b_32', \n}","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:43.934922Z","iopub.status.idle":"2023-10-20T05:30:43.935348Z","shell.execute_reply.started":"2023-10-20T05:30:43.935128Z","shell.execute_reply":"2023-10-20T05:30:43.935148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_obj = ViT(config)\nmodel = Classifier(model_obj)\n\ncheckpoint_callback.dirpath = os.path.join(config['dir'], 'ckpts')\ncheckpoint_callback.filename = config['ckpt_file_name']\n\nlogger = TensorBoardLogger(vit_b_32_dir+\"/tb_logs\")\n\ntrainer = Trainer(callbacks=[early_stop_callback, checkpoint_callback, rich_progress_bar, rich_model_summary], \n                  accelerator = 'cpu' ,max_epochs=1, logger=[logger])  \n \ntrainer.fit(model, tr_loader, val_loader)","metadata":{"execution":{"iopub.status.busy":"2023-10-20T05:30:43.936887Z","iopub.status.idle":"2023-10-20T05:30:43.937313Z","shell.execute_reply.started":"2023-10-20T05:30:43.937097Z","shell.execute_reply":"2023-10-20T05:30:43.937116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}